# Final notebook placeholder. Copy from Colab into here.
# Step 1: Import Libraries

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: Load the Dataset
df = pd.read_csv("synthetic_food_dataset_imbalanced.csv")

# Step 3: Preview the Data
print("Shape of Dataset:", df.shape)
df.head()

# Step 4: Check for Nulls and Duplicates
print("\nMissing values:\n", df.isnull().sum())
print("\nDuplicate rows:", df.duplicated().sum())

# Step 5: Basic Statistics
print("\nDataset Info:")
df.info()

print("\nSummary Statistics:")
print(df.describe())

# Step 6: Class Distribution
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='Category', order=df['Category'].value_counts().index)
plt.title("Class Distribution of Food Categories")
plt.xticks(rotation=45)
plt.show()

import pandas as pd
df = pd.read_csv("synthetic_food_dataset_imbalanced.csv")
df.head()
print("Column Names in Dataset:")
print(df.columns.tolist())
from google.colab import files
uploaded = files.upload()
import pandas as pd

df = pd.read_csv('synthetic_food_dataset_imbalanced.csv')
df.head()
print(df.columns.tolist())     # Show column names
df.info()                      # Check structure
df.describe()                  # Summary stats
Step 1: Handle Missing Values
print("Missing values per column:")
print(df.isnull().sum())

# Drop rows with missing values (or use imputation if you prefer)
df = df.dropna()

# Step 2: Remove Duplicate Rows
df = df.drop_duplicates()

# Step 3: Encode Categorical Features (Target: 'Meal_Type')
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Meal_Type_Label'] = le.fit_transform(df['Meal_Type'])

# Step 4: Select Features (Numerical only for now)
X = df[['Calories', 'Protein', 'Fat', 'Carbs', 'Sugar', 'Fiber',
        'Sodium', 'Cholesterol', 'Glycemic_Index', 'Water_Content', 'Serving_Size']]
y = df['Meal_Type_Label']

# Confirm shapes
print("Features shape:", X.shape)
print("Target shape:", y.shape)
Step 1: Split the dataset
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Import models and metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Dictionary of models
models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC()
}

# Step 3: Train and evaluate each model
for name, model in models.items():
    print(f"\nðŸ”¸ Model: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    # Metrics
    acc = accuracy_score(y_test, y_pred)
    print("Accuracy:", acc)
    print("Classification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_))
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()
Feature importance from Random Forest
import numpy as np

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Plot feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 5))
sns.barplot(x=[X.columns[i] for i in indices], y=importances[indices])
plt.title("Feature Importance (Random Forest)")
plt.xlabel("Features")
plt.ylabel("Importance Score")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
df.columns

